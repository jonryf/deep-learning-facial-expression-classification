{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import os, random, copy\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define sigmoid and its derivative for activation & backprop\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def derivSigmoid(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 - Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotions: ['fear', 'surprise', 'sadness', 'happiness', 'anger', 'disgust'] \n",
      "\n",
      "fear: 25 # of images\n",
      "surprise: 83 # of images\n",
      "sadness: 28 # of images\n",
      "happiness: 69 # of images\n",
      "anger: 45 # of images\n",
      "disgust: 59 # of images\n",
      "\n",
      "Balanced Set:\n",
      "happiness: 45 # of images\n",
      "anger: 45 # of images\n",
      "Converting from array to PIL Image\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# CSE 253: Programming Assignment 1\n",
    "# Code snippet by Michael\n",
    "# Winter 2020\n",
    "################################################################################\n",
    "# We've provided you with the dataset in PA1.zip\n",
    "################################################################################\n",
    "# To install PIL, refer to the instructions for your system:\n",
    "# https://pillow.readthedocs.io/en/5.2.x/installation.html\n",
    "################################################################################\n",
    "# If you don't have NumPy installed, please use the instructions here:\n",
    "# https://scipy.org/install.html\n",
    "################################################################################\n",
    "\n",
    "\n",
    "''' \n",
    "list of face expressions (contempt, neutral are excluded) are:\n",
    "1. anger\n",
    "2. disgust\n",
    "3. fear\n",
    "4. happiness\n",
    "5. sadness\n",
    "6. surprise\n",
    "'''\n",
    "\n",
    "def load_data(data_dir=\"./aligned/\"):\n",
    "\t\"\"\" Load all PNG images stored in your data directory into a list of NumPy\n",
    "\tarrays.\n",
    "\n",
    "\tArgs:\n",
    "\t\tdata_dir: The relative directory path to the CK+ image directory.\n",
    "\tReturns:\n",
    "\t\timages: A dictionary with keys as emotions and a list containing images associated with each key.\n",
    "\t\tcnt: A dictionary that stores the # of images in each emotion\n",
    "\t\"\"\"\n",
    "\timages = defaultdict(list)\n",
    "\n",
    "\t# Get the list of emotional directory:\n",
    "\tfor e in listdir(data_dir):\n",
    "\t\t# excluding any non-directory files\n",
    "\t\tif not os.path.isdir(os.path.join(data_dir, e)):\n",
    "\t\t\tcontinue\n",
    "\t\t# Get the list of image file names\n",
    "\t\tall_files = listdir(os.path.join(data_dir, e))\n",
    "\n",
    "\t\tfor file in all_files:\n",
    "\t\t\t# Load only image files as PIL images and convert to NumPy arrays\n",
    "\t\t\tif '.png' in file:\n",
    "\t\t\t\timg = Image.open(os.path.join(data_dir, e, file))\n",
    "\t\t\t\timages[e].append(np.array(img))\n",
    "\n",
    "\tprint(\"Emotions: {} \\n\".format(list(images.keys())))\n",
    "\n",
    "\tcnt = defaultdict(int)\n",
    "\tfor e in images.keys():\n",
    "\t\tprint(\"{}: {} # of images\".format(e, len(images[e])))\n",
    "\t\tcnt[e] = len(images[e])\n",
    "\treturn images, cnt\n",
    "\n",
    "def balanced_sampler(dataset, cnt, emotions):\n",
    "\t# this ensures everyone has the same balanced subset for model training, don't change this seed value\n",
    "\trandom.seed(20)\n",
    "\tprint(\"\\nBalanced Set:\")\n",
    "\tmin_cnt = min([cnt[e] for e in emotions])\n",
    "\tbalanced_subset = defaultdict(list)\n",
    "\tfor e in emotions:\n",
    "\t\tbalanced_subset[e] = copy.deepcopy(dataset[e])\n",
    "\t\trandom.shuffle(balanced_subset[e])\n",
    "\t\tbalanced_subset[e] = balanced_subset[e][:min_cnt]\n",
    "\t\tprint('{}: {} # of images'.format(e, len(balanced_subset[e])))\n",
    "\treturn balanced_subset\n",
    "\n",
    "def display_face(img):\n",
    "\t\"\"\" Display the input image and optionally save as a PNG.\n",
    "\n",
    "\tArgs:\n",
    "\t\timg: The NumPy array or image to display\n",
    "\n",
    "\tReturns: None\n",
    "\t\"\"\"\n",
    "\t# Convert img to PIL Image object (if it's an ndarray)\n",
    "\tif type(img) == np.ndarray:\n",
    "\t\tprint(\"Converting from array to PIL Image\")\n",
    "\t\timg = Image.fromarray(img)\n",
    "\n",
    "\t# Display the image\n",
    "\timg.show()\n",
    "\n",
    "\n",
    "# example on how to use it\n",
    "if __name__ == '__main__':\n",
    "\t# The relative path to your image directory\n",
    "\tdata_dir = \"./aligned/\"\n",
    "\tdataset, cnt = load_data(data_dir)\n",
    "\t# test with happiness and anger\n",
    "\timages = balanced_sampler(dataset, cnt, emotions=['happiness', 'anger'])\n",
    "\tdisplay_index = 0\n",
    "\tdisplay_face(images['anger'][display_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10 # number of sections after folding\n",
    "M = 50 # maximum number of epochs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list happiness as a 1, anger as a 0\n",
    "X = images['happiness'] + images['anger']\n",
    "y = [1] * 45 + [0] * 45\n",
    "\n",
    "#get my feature - label pairs zipped together\n",
    "all_happy_angry = list(zip(X,y))\n",
    "\n",
    "#randomize the dataset so I can fold properly\n",
    "\n",
    "random.shuffle(all_happy_angry)\n",
    "\n",
    "def kfold(data, k=10):\n",
    "    folds = []\n",
    "    size = int((1/k) * len(data))\n",
    "    for i in range(0,k):\n",
    "        folds.append(data[i*size : (i+1)*size])\n",
    "    return folds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "folds = kfold(all_happy_angry)\n",
    "print(len(folds[0]) == len(folds[1]))\n",
    "print(len(folds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5 - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \n",
    "    def __init__(self, lr, dim):\n",
    "        self.lr = lr\n",
    "        self.w = np.zeros(dim) \n",
    "    \n",
    "    def stochastic_gradient_descent(self, X, labels):\n",
    "        indices = [i for i in range(len(labels))]\n",
    "        np.random.shuffle(indices)\n",
    "        for i in indices:\n",
    "            \n",
    "            # make predition\n",
    "            data = X[i]\n",
    "            label = labels[i]\n",
    "            predicted = sigmoid(data.dot(self.w))\n",
    "            error = label - predicted\n",
    "            \n",
    "            # update weights\n",
    "            for i in range(len(self.w)):\n",
    "                grad = error * data[i]\n",
    "                self.w[i] += self.lr * grad\n",
    "            \n",
    "    def probabilities(self, X):\n",
    "        return sigmoid(X.dot(self.w)).reshape(-1)\n",
    "\n",
    "    \n",
    "    def batch_gradient_descent(self, X, labels):\n",
    "        predicted = self.probabilities(X)\n",
    "        error = labels - predicted\n",
    "        grad = X.T.dot(error)\n",
    "        self.w += self.lr * grad\n",
    "        \n",
    "\n",
    "    def loss(self, labels, predicted):\n",
    "        #cost = np.log(predicted+ 0.0001) * labels + (1 - labels) * np.log(1 - predicted)\n",
    "        #return - cost.sum() / len(labels)\n",
    "        #print(cost)\n",
    "        \n",
    "        Yis1 = labels == 1\n",
    "        cost = -(np.log(predicted[Yis1]).sum() + np.log(1 - predicted[~Yis1]).sum())/len(labels)\n",
    "        return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(0.1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.loss(np.array([1,1]), np.array([1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11157177565710485"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.loss(np.array([1,1]), np.array([0.8,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.array([[0.9,0.22,0.44], [0.0,0.22,0.44]])\n",
    "\n",
    "for i in range(50):\n",
    "    lr.stochastic_gradient_descent(test_data, np.array([0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.52246431,  0.16109949,  0.32219898])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.23272131, 0.54418679])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.probabilities(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4366839636474693"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.loss(np.array([0, 1]), lr.probabilities(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 1, 0]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5 \n",
    "\n",
    "[0, 0, 0, 0, 1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_mapping = [\"fear\", \"surprise\", \"sadness\", \"happiness\", \"anger\", \"disgust\"]\n",
    "\n",
    "\n",
    "class SoftmaxRegression:\n",
    "    \n",
    "    def __init__(self, lr, dim, c):\n",
    "        self.lr = lr\n",
    "        self.c = c\n",
    "        self.w = np.zeros((dim, c)) \n",
    "    \n",
    "    def stochastic_gradient_descent(self, X, labels):\n",
    "        indices = [i for i in range(len(labels))]\n",
    "        np.random.shuffle(indices)\n",
    "        for i in indices:\n",
    "            \n",
    "            # make predition\n",
    "            data = X[i]\n",
    "            label = labels[i]\n",
    "            predicted = self.probabilities(data)\n",
    "            error = label - predicted\n",
    "            \n",
    "            # update weights\n",
    "            for i in range(len(self.w)):\n",
    "                grad = error * data[i]\n",
    "                self.w[i] += self.lr * grad\n",
    "            \n",
    "    def probabilities(self, X):\n",
    "        return np.exp(X.dot(self.w)) / np.sum(np.exp(X.dot(self.w)), axis=1)\n",
    "\n",
    "    \n",
    "    def batch_gradient_descent(self, X, labels):\n",
    "        predicted = self.probabilities(X)\n",
    "        error = labels - predicted\n",
    "        grad = X.T.dot(error)\n",
    "        #grad = np.sum(grad, axis=1).reshape(grad.shape[0], 1)\n",
    "        self.w += self.lr * grad\n",
    "        \n",
    "\n",
    "    def loss(self, labels, predicted):\n",
    "        #return -np.sum(np.sum(labels.T.dot(np.log(predicted)))) # can't have zero values \n",
    "        sum_of_score = 0\n",
    "        for n in range(predicted.shape[0]):\n",
    "            for cat in range(self.c):\n",
    "                sum_of_score += labels[n, cat] * np.log(predicted[n, cat])\n",
    "\n",
    "        return - sum_of_score / predicted.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = SoftmaxRegression(0.02, 4, 2)\n",
    "\n",
    "test_data = np.array([[0.3,0.22,0.44, 0.44], [0.0,0.12,0.2, 0.2]])\n",
    "\n",
    "\n",
    "\n",
    "#softmax.loss(np.array([[1, 0],[1,0]]), softmax.probabilities(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01005033585350145"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax.loss(np.array([[1, 0],[1,0]]), np.array([[0.99,0.01], [0.99,0.01]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    softmax.batch_gradient_descent(test_data, np.array([[1, 0],[0,0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.13271808, -0.13035603],\n",
       "       [ 0.03345672, -0.15292185],\n",
       "       [ 0.0882034 , -0.28673456],\n",
       "       [ 0.0882034 , -0.28673456]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.61060854, 0.3771708 ],\n",
       "       [0.56057181, 0.45702117]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax.probabilities(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA:\n",
    "    def __init__(self, square_data, k=10): # k defaults to 10\n",
    "        self.square_data = square_data\n",
    "        M, rows, columns = self.square_data.shape\n",
    "        self.k = k\n",
    "        self.num_examples = M\n",
    "        self.image_vec = np.reshape(square_data, (M, rows*columns))\n",
    "        self.mean_face = np.mean(self.image_vec, axis=0)\n",
    "        self.std_face = np.std(self.image_vec, axis=0)\n",
    "        self.components, self.singular_values = self.get_components()\n",
    "    \n",
    "    def get_components(self):\n",
    "        Phi = (self.image_vec - self.mean_face) / self.std_face \n",
    "        A = Phi\n",
    "        C = np.matmul(A, A.T)\n",
    "        C = np.divide(C, self.num_examples - 1)\n",
    "        evals, Vi = np.linalg.eigh(C)\n",
    "        z = list(zip(evals,Vi))\n",
    "        z.sort(reverse=True)        \n",
    "        \n",
    "        idx = np.argsort(evals)[::-1]\n",
    "        evecs = Vi[:,idx]\n",
    "        evecs = evecs[:, :self.k]\n",
    "        pc = evecs\n",
    "        \n",
    "        #final components (num pixels by k matrix)\n",
    "        components = np.matmul(A.T, pc)\n",
    "        components = components / np.linalg.norm(components, axis=0)\n",
    "        #get singluar values\n",
    "        sorted_evals = np.array([x[0] for x in z])\n",
    "        postive_evals = sorted_evals[:self.k]\n",
    "        singular_values = np.sqrt(postive_evals.reshape(1, -1))\n",
    "        assert np.allclose(np.linalg.norm(components, axis=0), 1)\n",
    "\n",
    "        return components, singular_values\n",
    "        \n",
    "    def transform(self, images):\n",
    "        return np.array([self.transform_single(i) for i in images])\n",
    "        \n",
    "    def transform_single(self, image): #take an image, and pc's, and output compressed image\n",
    "        image =  image.reshape(1, -1) \n",
    "        image = (image - self.mean_face) / self.std_face\n",
    "        compressed_image_vectors = np.matmul(image, self.components) / self.singular_values\n",
    "        return compressed_image_vectors.reshape(-1,)\n",
    "    \n",
    "    '''\n",
    "    def transform(self, images): #take an image, and pc's, and output compressed image\n",
    "        M, rows, columns = images.shape\n",
    "        flat_images = (np.reshape(images, (M, rows*columns)) - self.mean_face) / self.std_face\n",
    "        print(\"Shape flat images\")\n",
    "        print(flat_images.shape)\n",
    "        compressed_image_vectors = np.matmul(flat_images, self.components) / self.singular_values\n",
    "        return compressed_image_vectors\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n",
      "Training index: 2\n",
      "Training index: 3\n",
      "Training index: 4\n",
      "Training index: 5\n",
      "Training index: 6\n",
      "Training index: 7\n",
      "Training index: 8\n",
      "Training index: 9\n",
      "(9, 224, 192)\n",
      "(9, 224, 192)\n",
      "(72, 224, 192)\n",
      "[[50 50 51 ... 61 60 60]\n",
      " [50 49 50 ... 61 62 63]\n",
      " [51 51 51 ... 63 64 66]\n",
      " ...\n",
      " [40 40 40 ... 48 48 49]\n",
      " [41 41 40 ... 46 46 45]\n",
      " [40 39 38 ... 49 49 48]]\n",
      "PCA transformed\n",
      "[ 0.49845016  0.94623586 -0.05710495  0.25494645 -1.41748828  0.11601997\n",
      " -0.59675826  0.83598796  0.01925483 -0.31428448]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (72,2) (72,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-06385783758e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-06385783758e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_gradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;31m#print(model.probabilities(train_data[0]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mtraining_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobabilities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-38710cf43de8>\u001b[0m in \u001b[0;36mbatch_gradient_descent\u001b[0;34m(self, X, labels)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbatch_gradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobabilities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-38710cf43de8>\u001b[0m in \u001b[0;36mprobabilities\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprobabilities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (72,2) (72,) "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#from pca import PCA\n",
    "\n",
    "EPOCHS = 100\n",
    "K = 10\n",
    "LOGISTIC = False\n",
    "CATEGORIES = 2\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "class EpochData:\n",
    "    def __init__(self):\n",
    "        self.acc = []\n",
    "        self.error = []\n",
    "    \n",
    "    def save(self, error, acc):\n",
    "        self.error.append(error)\n",
    "        self.acc.append(acc)\n",
    "        \n",
    "    \n",
    "    def score(self):\n",
    "        return self.error[-1]\n",
    "        \n",
    "        \n",
    "def transform(pca, data):\n",
    "    labels = data[1]\n",
    "    if not LOGISTIC: # if softmax\n",
    "        labels = one_hot_encode(labels)\n",
    "    return pca.transform(data[0]), labels\n",
    "    #np.array([pca.transform(i)[0] for i in data[0]]), labels\n",
    "\n",
    "def visualize_data(plots, legends, x_label, y_label):\n",
    "    x = np.arange(1, len(plots[0]) + 1)\n",
    "    for data in plots:\n",
    "        plt.plot(x, data)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.legend(legends)\n",
    "    plt.show()\n",
    "    \n",
    "def split_x_y(data):\n",
    "    return np.array([item[0] for item in data]), np.array([item[1] for item in data]) \n",
    "\n",
    "def one_hot_encode(labels):\n",
    "    new_labels = np.zeros((len(labels), CATEGORIES))\n",
    "    new_labels[np.arange(labels.size), labels] = 1\n",
    "    return new_labels\n",
    "\n",
    "def train():\n",
    "    best_model = None\n",
    "    \n",
    "    folds = kfold(all_happy_angry)\n",
    "    k = len(folds)\n",
    "    for fold in range(k):\n",
    "        # define the model\n",
    "        if LOGISTIC:\n",
    "            model = LogisticRegression(LEARNING_RATE, K)\n",
    "        else:\n",
    "            model = SoftmaxRegression(LEARNING_RATE, K, CATEGORIES)\n",
    "\n",
    "        # split data\n",
    "        val_data, test_data = split_x_y(folds[fold]), split_x_y(folds[(fold + 1) % k])\n",
    "        train_data = None\n",
    "        print(\"Fold: \" + str(fold))\n",
    "        for i in range(k):\n",
    "            if i != fold and i != ((fold + 1) % k):\n",
    "                print(\"Training index: \" + str(i))\n",
    "                if train_data is None:\n",
    "                    train_data = folds[i]\n",
    "                else:\n",
    "                    train_data = np.concatenate((train_data, folds[i]))\n",
    "        train_data = split_x_y(train_data)\n",
    "        print(val_data[0].shape)\n",
    "        print(test_data[0].shape)\n",
    "        print(train_data[0].shape)\n",
    "\n",
    "        print(train_data[0][0])\n",
    "        pca = PCA(train_data[0], K) # PCA(10) #\n",
    "        #pca.fit(train_data[0])\n",
    "        # PCA and one_hot\n",
    "        train_data, test_data, val_data = transform(pca, train_data), transform(pca, test_data), transform(pca, val_data)        \n",
    "        print(\"PCA transformed\")\n",
    "        print(train_data[0][0])\n",
    "        validation_performance = EpochData()\n",
    "        training_performance = EpochData() \n",
    "        \n",
    "        \n",
    "        for epoch in range(EPOCHS):\n",
    "            model.batch_gradient_descent(train_data[0], train_data[1])\n",
    "            #print(model.probabilities(train_data[0]))\n",
    "            training_error = model.loss(train_data[1], model.probabilities(train_data[0]))\n",
    "            validation_error = model.loss(val_data[1], model.probabilities(val_data[0]))\n",
    "            \n",
    "            traning_acc = 0.3 #model.make_prediction()\n",
    "            validation_acc = 0.4 #model.make_prediction()\n",
    "            \n",
    "            print(\"Training error: {}, validation error: {}, accuracy: {}\".format(training_error, validation_error, traning_acc))\n",
    "        \n",
    "            # save\n",
    "            validation_performance.save(validation_error, validation_acc)\n",
    "            training_performance.save(training_error, traning_acc)\n",
    "        \n",
    "        \n",
    "        # plot the graphs\n",
    "        data_to_plot = [training_performance.error, validation_performance.error]\n",
    "        legends = [\"Training error\", \"Validation error\"]\n",
    "        visualize_data(data_to_plot,legends, \"Epoch\", \"Cross entropy error\")\n",
    "                \n",
    "        # save the validation data to the model\n",
    "        model.epoch_data = validation_performance \n",
    "        \n",
    "        # save the best model\n",
    "        if best_model is None:\n",
    "            best_model = model\n",
    "        elif best_model.epoch_data.score() > model.epoch_data.score():\n",
    "            best_model = model\n",
    "        \n",
    "            \n",
    "\n",
    "train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.array([\n",
    "    [[40, 33, 34], \n",
    "     [54, 34, 22]],\n",
    "    \n",
    "    [[34, 40, 34], \n",
    "     [23, 32, 32]],\n",
    "\n",
    "    [[60, 30, 64], \n",
    "     [73, 62, 72]]])\n",
    "\n",
    "test_img = np.array([[34, 36, 45], [34, 36, 45]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PCA2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-2e7191560b12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'PCA2' is not defined"
     ]
    }
   ],
   "source": [
    "pca = PCA2(np.array(test_data), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.transform_single(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.singular_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pca import PCA\n",
    "\n",
    "pca1 = PCA(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca1.fit(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca1.transform(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca1.p_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca1.s_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
